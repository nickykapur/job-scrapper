name: Parallel Job Scraper (Fast)

on:
  schedule:
    # Runs at 9 AM, 11 AM, 1 PM, 3 PM, 4 PM, 6 PM, 8 PM Dublin time
    - cron: '0 9,11,13,15,16,18,20 * * *'

  workflow_dispatch:

jobs:
  scrape-country:
    runs-on: ubuntu-latest

    # Run 7 countries in parallel
    strategy:
      matrix:
        country:
          - { location: "Dublin, County Dublin, Ireland", name: "Ireland" }
          - { location: "Madrid, Community of Madrid, Spain", name: "Spain" }
          - { location: "Panama City, Panama", name: "Panama" }
          - { location: "Santiago, Chile", name: "Chile" }
          - { location: "Amsterdam, North Holland, Netherlands", name: "Netherlands" }
          - { location: "Berlin, Germany", name: "Germany" }
          - { location: "Stockholm, Sweden", name: "Sweden" }
      max-parallel: 7  # Run all 7 at once
      fail-fast: false  # Continue even if one fails

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper for ${{ matrix.country.name }}
        run: |
          python daily_single_country_scraper.py \
            --location "${{ matrix.country.location }}" \
            --country "${{ matrix.country.name }}"
        env:
          RAILWAY_URL: https://web-production-110bb.up.railway.app

  # Cleanup job - runs after all countries finish
  cleanup:
    runs-on: ubuntu-latest
    needs: scrape-country
    if: always()  # Run even if some countries failed

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Enforce country job limits
        id: cleanup
        run: |
          echo "Running cleanup..."
          RESPONSE=$(curl -s -X POST https://web-production-110bb.up.railway.app/api/jobs/enforce-country-limit \
            -H "Content-Type: application/json" \
            -d '{"max_jobs": 300}')
          echo "$RESPONSE" | python3 -m json.tool

          # Extract stats for notification
          JOBS_DELETED=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('jobs_deleted', 0))")
          TOTAL_JOBS=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('total_jobs_remaining', 0))")

          echo "jobs_deleted=$JOBS_DELETED" >> $GITHUB_OUTPUT
          echo "total_jobs=$TOTAL_JOBS" >> $GITHUB_OUTPUT

      - name: Send notification (Discord/Slack)
        if: env.WEBHOOK_URL != ''
        env:
          WEBHOOK_URL: ${{ secrets.NOTIFICATION_WEBHOOK_URL }}
        run: |
          # Get job results from previous step
          JOBS_DELETED="${{ steps.cleanup.outputs.jobs_deleted }}"
          TOTAL_JOBS="${{ steps.cleanup.outputs.total_jobs }}"

          # Count successful vs failed countries
          if [ "${{ needs.scrape-country.result }}" == "success" ]; then
            STATUS="‚úÖ All countries completed"
            COLOR="3066993"
          else
            STATUS="‚ö†Ô∏è Some countries failed"
            COLOR="15158332"
          fi

          # Create notification payload
          PAYLOAD=$(cat <<EOF
          {
            "embeds": [{
              "title": "Job Scraper Complete",
              "description": "$STATUS",
              "color": $COLOR,
              "fields": [
                {"name": "Total Jobs", "value": "$TOTAL_JOBS", "inline": true},
                {"name": "Jobs Cleaned", "value": "$JOBS_DELETED", "inline": true},
                {"name": "Duration", "value": "${{ github.run_number }} run", "inline": true}
              ],
              "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
            }]
          }
          EOF
          )

          # Send to webhook
          curl -X POST "$WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d "$PAYLOAD"

      - name: Summary comment
        run: |
          echo "### üéâ Scraping Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Jobs:** ${{ steps.cleanup.outputs.total_jobs }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Jobs Cleaned:** ${{ steps.cleanup.outputs.jobs_deleted }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** All 7 countries processed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "View jobs: https://web-production-110bb.up.railway.app" >> $GITHUB_STEP_SUMMARY

# Speed: 7 parallel jobs √ó 5 minutes = ~5-7 minutes total
# PUBLIC repos get UNLIMITED GitHub Actions minutes - completely FREE!
# Usage: 7 jobs √ó 5 min √ó 7 runs/day √ó 30 days = 7350 minutes/month
# Cost: $0 (FREE for public repositories)

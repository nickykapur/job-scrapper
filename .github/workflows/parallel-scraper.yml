name: Parallel Job Scraper (Fast)

on:
  schedule:
    # Runs at 9 AM, 11 AM, 1 PM, 3 PM, 4 PM, 6 PM, 8 PM Dublin time
    - cron: '0 9,11,13,15,16,18,20 * * *'

  workflow_dispatch:

jobs:
  scrape-country:
    runs-on: ubuntu-latest

    # Run 9 countries in parallel
    strategy:
      matrix:
        country:
          - { location: "Dublin, County Dublin, Ireland", name: "Ireland" }
          - { location: "Madrid, Community of Madrid, Spain", name: "Spain" }
          - { location: "Panama City, Panama", name: "Panama" }
          - { location: "Santiago, Chile", name: "Chile" }
          - { location: "Amsterdam, North Holland, Netherlands", name: "Netherlands" }
          - { location: "Berlin, Germany", name: "Germany" }
          - { location: "Stockholm, Sweden", name: "Sweden" }
          - { location: "Brussels, Belgium", name: "Belgium" }
          - { location: "Copenhagen, Denmark", name: "Denmark" }
      max-parallel: 9  # Run all 9 at once
      fail-fast: false  # Continue even if one fails

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper for ${{ matrix.country.name }}
        id: scrape
        run: |
          python daily_single_country_scraper.py \
            --location "${{ matrix.country.location }}" \
            --country "${{ matrix.country.name }}"
        env:
          RAILWAY_URL: https://web-production-110bb.up.railway.app

      - name: Send Slack notification
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          # Log to console
          if [ "${{ steps.scrape.outcome }}" == "success" ]; then
            SOFTWARE_JOBS="${{ steps.scrape.outputs.software_jobs }}"
            HR_JOBS="${{ steps.scrape.outputs.hr_jobs }}"
            TOTAL_JOBS="${{ steps.scrape.outputs.jobs_found }}"
            echo "[SUCCESS] ${{ matrix.country.name }}: $TOTAL_JOBS new jobs (Software: $SOFTWARE_JOBS, HR: $HR_JOBS)"

            # Send to Slack if configured
            if [ -n "$SLACK_WEBHOOK_URL" ]; then
              # Different message based on whether new jobs were found
              if [ "$TOTAL_JOBS" -gt "0" ]; then
                MESSAGE="*${{ matrix.country.name }}*: $SOFTWARE_JOBS software | $HR_JOBS HR positions"
              else
                MESSAGE="*${{ matrix.country.name }}*: No new jobs found"
              fi

              curl -X POST "$SLACK_WEBHOOK_URL" \
                -H "Content-Type: application/json" \
                -d "{\"text\":\"$MESSAGE\"}" \
                > /dev/null 2>&1
            fi
          else
            echo "[FAILED] ${{ matrix.country.name }}: Scraping failed"

            # Send failure to Slack
            if [ -n "$SLACK_WEBHOOK_URL" ]; then
              curl -X POST "$SLACK_WEBHOOK_URL" \
                -H "Content-Type: application/json" \
                -d "{\"text\":\"*${{ matrix.country.name }}*: Scraping failed\"}" \
                > /dev/null 2>&1
            fi
          fi

  # Cleanup job - runs after all countries finish
  cleanup:
    runs-on: ubuntu-latest
    needs: scrape-country
    if: always()  # Run even if some countries failed

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Enforce country job limits
        id: cleanup
        run: |
          echo "Running cleanup..."
          RESPONSE=$(curl -s -X POST https://web-production-110bb.up.railway.app/api/jobs/enforce-country-limit \
            -H "Content-Type: application/json" \
            -d '{"max_jobs": 300}')
          echo "$RESPONSE" | python3 -m json.tool

          # Extract stats for notification
          JOBS_DELETED=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('jobs_deleted', 0))")
          TOTAL_JOBS=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('total_jobs_remaining', 0))")

          echo "jobs_deleted=$JOBS_DELETED" >> $GITHUB_OUTPUT
          echo "total_jobs=$TOTAL_JOBS" >> $GITHUB_OUTPUT

      - name: Create summary
        run: |
          # Create GitHub Actions summary (visible in workflow UI)
          echo "### Job Scraping Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Jobs in Database:** ${{ steps.cleanup.outputs.total_jobs }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Jobs Removed:** ${{ steps.cleanup.outputs.jobs_deleted }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Countries Processed:** 9" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Links" >> $GITHUB_STEP_SUMMARY
          echo "- [View Dashboard](https://web-production-110bb.up.railway.app)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Next run: Check cron schedule" >> $GITHUB_STEP_SUMMARY

# Speed: 7 parallel jobs × 5 minutes = ~5-7 minutes total
# PUBLIC repos get UNLIMITED GitHub Actions minutes - completely FREE!
# Usage: 7 jobs × 5 min × 7 runs/day × 30 days = 7350 minutes/month
# Cost: $0 (FREE for public repositories)

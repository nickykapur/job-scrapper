name: Parallel Job Scraper (Fast)

on:
  schedule:
    # Runs at 9 AM, 11 AM, 1 PM, 3 PM, 4 PM, 6 PM, 8 PM Dublin time
    - cron: '0 9,11,13,15,16,18,20 * * *'

  workflow_dispatch:

jobs:
  scrape-country:
    runs-on: ubuntu-latest

    # Run 11 countries in parallel (all job types: software, HR, cybersecurity, sales, finance)
    strategy:
      matrix:
        country:
          - { location: "Dublin, County Dublin, Ireland", name: "Ireland" }
          - { location: "London, England, United Kingdom", name: "United Kingdom" }
          - { location: "Madrid, Community of Madrid, Spain", name: "Spain" }
          - { location: "Panama City, Panama", name: "Panama" }
          - { location: "Santiago, Chile", name: "Chile" }
          - { location: "Amsterdam, North Holland, Netherlands", name: "Netherlands" }
          - { location: "Berlin, Germany", name: "Germany" }
          - { location: "Stockholm, Sweden", name: "Sweden" }
          - { location: "Brussels, Belgium", name: "Belgium" }
          - { location: "Copenhagen, Denmark", name: "Denmark" }
          - { location: "Luxembourg City, Luxembourg", name: "Luxembourg" }
      max-parallel: 11  # Run all 11 at once
      fail-fast: false  # Continue even if one fails

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper for ${{ matrix.country.name }}
        id: scrape
        run: |
          python daily_single_country_scraper.py \
            --location "${{ matrix.country.location }}" \
            --country "${{ matrix.country.name }}"
        env:
          RAILWAY_URL: https://web-production-110bb.up.railway.app

      - name: Log results
        if: always()
        run: |
          if [ "${{ steps.scrape.outcome }}" == "success" ]; then
            SOFTWARE_JOBS="${{ steps.scrape.outputs.software_jobs }}"
            HR_JOBS="${{ steps.scrape.outputs.hr_jobs }}"
            CYBER_JOBS="${{ steps.scrape.outputs.cybersecurity_jobs }}"
            SALES_JOBS="${{ steps.scrape.outputs.sales_jobs }}"
            FINANCE_JOBS="${{ steps.scrape.outputs.finance_jobs }}"
            TOTAL_JOBS="${{ steps.scrape.outputs.jobs_found }}"
            echo "[SUCCESS] ${{ matrix.country.name }}: $TOTAL_JOBS new jobs (Software: $SOFTWARE_JOBS, HR: $HR_JOBS, Cybersecurity: $CYBER_JOBS, Sales: $SALES_JOBS, Finance: $FINANCE_JOBS)"
          else
            echo "[FAILED] ${{ matrix.country.name }}: Scraping failed"
          fi

  # Cleanup and Summary - runs after all countries finish
  cleanup:
    runs-on: ubuntu-latest
    needs: scrape-country
    if: always()  # Run even if some countries failed

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Enforce country job limits
        id: cleanup
        run: |
          echo "Running cleanup..."
          RESPONSE=$(curl -s -X POST https://web-production-110bb.up.railway.app/api/jobs/enforce-country-limit \
            -H "Content-Type: application/json" \
            -d '{"max_jobs": 300}')
          echo "$RESPONSE" | python3 -m json.tool

          # Extract stats for notification
          JOBS_DELETED=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('jobs_deleted', 0))")
          TOTAL_JOBS=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('total_jobs_remaining', 0))")

          echo "jobs_deleted=$JOBS_DELETED" >> $GITHUB_OUTPUT
          echo "total_jobs=$TOTAL_JOBS" >> $GITHUB_OUTPUT

      - name: Get job statistics
        id: stats
        run: |
          # Get job counts by country and type
          STATS=$(curl -s "https://web-production-110bb.up.railway.app/api/jobs/stats")
          echo "$STATS" | python3 -m json.tool

          # Parse stats (you'll need to add this endpoint)
          echo "stats_json<<EOF" >> $GITHUB_OUTPUT
          echo "$STATS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Send consolidated Slack notification
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -z "$SLACK_WEBHOOK_URL" ]; then
            echo "SLACK_WEBHOOK_URL not configured, skipping notification"
            exit 0
          fi

          # Build consolidated message
          MESSAGE=$(cat <<EOF
          {
            "blocks": [
              {
                "type": "header",
                "text": {
                  "type": "plain_text",
                  "text": "Job Scraping Complete"
                }
              },
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "*Run Summary*\n:white_check_mark: All 11 countries processed\n:clock3: Duration: ~6 minutes"
                }
              },
              {
                "type": "section",
                "fields": [
                  {"type": "mrkdwn", "text": "*Total Jobs:*\n${{ steps.cleanup.outputs.total_jobs }}"},
                  {"type": "mrkdwn", "text": "*Cleaned:*\n${{ steps.cleanup.outputs.jobs_deleted }}"}
                ]
              },
              {
                "type": "divider"
              },
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "*Countries Scraped:*\n:flag-ie: Ireland | :flag-gb: UK | :flag-es: Spain | :flag-pa: Panama | :flag-cl: Chile | :flag-nl: Netherlands\n:flag-de: Germany | :flag-se: Sweden | :flag-be: Belgium | :flag-dk: Denmark | :flag-lu: Luxembourg"
                }
              },
              {
                "type": "actions",
                "elements": [
                  {
                    "type": "button",
                    "text": {
                      "type": "plain_text",
                      "text": "View Jobs"
                    },
                    "url": "https://web-production-110bb.up.railway.app"
                  }
                ]
              }
            ]
          }
          EOF
          )

          curl -X POST "$SLACK_WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d "$MESSAGE"

          echo "Consolidated notification sent to Slack"

      - name: Create summary
        run: |
          # Create GitHub Actions summary (visible in workflow UI)
          echo "### Job Scraping Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Jobs in Database:** ${{ steps.cleanup.outputs.total_jobs }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Jobs Removed:** ${{ steps.cleanup.outputs.jobs_deleted }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Countries Processed:** 11" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Links" >> $GITHUB_STEP_SUMMARY
          echo "- [View Dashboard](https://web-production-110bb.up.railway.app)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Next run: Check cron schedule" >> $GITHUB_STEP_SUMMARY

# Speed: 11 parallel jobs × 6 minutes = ~6-7 minutes total
# PUBLIC repos get UNLIMITED GitHub Actions minutes - completely FREE!
# Usage: 11 jobs × 6 min × 7 runs/day × 30 days = 13,860 minutes/month
# Cost: $0 (FREE for public repositories)
#
# Notifications: 1 consolidated message per run (7/day) + 1 daily digest (1/day) = 8 notifications/day total

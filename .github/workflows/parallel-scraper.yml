name: Optimized Job Scraper (Active Users Only)

on:
  schedule:
    # Runs at 9 AM, 11 AM, 1 PM, 3 PM, 4 PM, 6 PM, 8 PM Dublin time
    - cron: '0 9,11,13,15,16,18,20 * * *'

  workflow_dispatch:

jobs:
  # First job: Check if scraping is needed based on active users
  check-scraping-needed:
    runs-on: ubuntu-latest
    outputs:
      should_scrape: ${{ steps.check.outputs.should_scrape }}
      active_users: ${{ steps.check.outputs.active_users }}
      countries_json: ${{ steps.check.outputs.countries_json }}
      job_types: ${{ steps.check.outputs.job_types }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install requests

      - name: Check if scraping is needed
        id: check
        run: |
          echo "Checking active users and their preferences..."

          # Run the script to get scraping targets
          python get_scraping_targets.py > targets.json || EXIT_CODE=$?

          # Check exit code
          if [ "${EXIT_CODE:-0}" -eq 2 ]; then
            echo "No active users - skipping scraping"
            echo "should_scrape=false" >> $GITHUB_OUTPUT
            echo "active_users=0" >> $GITHUB_OUTPUT
            echo "countries_json=[]" >> $GITHUB_OUTPUT
            echo "job_types=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Parse the output
          cat targets.json
          ACTIVE_USERS=$(cat targets.json | python3 -c "import sys, json; print(json.load(sys.stdin).get('active_users_count', 0))")
          SHOULD_SCRAPE=$(cat targets.json | python3 -c "import sys, json; print('true' if not json.load(sys.stdin).get('skip_scraping', True) else 'false')")

          echo "Active users: $ACTIVE_USERS"
          echo "Should scrape: $SHOULD_SCRAPE"

          echo "should_scrape=$SHOULD_SCRAPE" >> $GITHUB_OUTPUT
          echo "active_users=$ACTIVE_USERS" >> $GITHUB_OUTPUT

          # Extract countries and job types for logging
          COUNTRIES=$(cat targets.json | python3 -c "import sys, json; print(', '.join([c['name'] for c in json.load(sys.stdin).get('countries', [])]))")
          JOB_TYPES=$(cat targets.json | python3 -c "import sys, json; print(', '.join([j['type'] for j in json.load(sys.stdin).get('job_types', [])]))")

          echo "countries_json=$COUNTRIES" >> $GITHUB_OUTPUT
          echo "job_types=$JOB_TYPES" >> $GITHUB_OUTPUT

          echo "Will scrape for: $JOB_TYPES in $COUNTRIES"
        env:
          API_BASE_URL: https://web-production-110bb.up.railway.app

  # Second job: Actually scrape (only if needed)
  scrape-country:
    runs-on: ubuntu-latest
    needs: check-scraping-needed
    if: needs.check-scraping-needed.outputs.should_scrape == 'true'

    # Run 11 countries in parallel (only active user preferences will be scraped)
    strategy:
      matrix:
        country:
          - { location: "Dublin, County Dublin, Ireland", name: "Ireland" }
          - { location: "London, England, United Kingdom", name: "United Kingdom" }
          - { location: "Madrid, Community of Madrid, Spain", name: "Spain" }
          - { location: "Panama City, Panama", name: "Panama" }
          - { location: "Santiago, Chile", name: "Chile" }
          - { location: "Amsterdam, North Holland, Netherlands", name: "Netherlands" }
          - { location: "Berlin, Germany", name: "Germany" }
          - { location: "Stockholm, Sweden", name: "Sweden" }
          - { location: "Brussels, Belgium", name: "Belgium" }
          - { location: "Copenhagen, Denmark", name: "Denmark" }
          - { location: "Luxembourg City, Luxembourg", name: "Luxembourg" }
      max-parallel: 11  # Run all 11 at once
      fail-fast: false  # Continue even if one fails

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run smart scraper for ${{ matrix.country.name }}
        id: scrape
        run: |
          # The scraper will check active user preferences internally
          python daily_single_country_scraper.py \
            --location "${{ matrix.country.location }}" \
            --country "${{ matrix.country.name }}"
        env:
          RAILWAY_URL: https://web-production-110bb.up.railway.app

      - name: Log results
        if: always()
        run: |
          if [ "${{ steps.scrape.outcome }}" == "success" ]; then
            SOFTWARE_JOBS="${{ steps.scrape.outputs.software_jobs }}"
            HR_JOBS="${{ steps.scrape.outputs.hr_jobs }}"
            CYBER_JOBS="${{ steps.scrape.outputs.cybersecurity_jobs }}"
            SALES_JOBS="${{ steps.scrape.outputs.sales_jobs }}"
            FINANCE_JOBS="${{ steps.scrape.outputs.finance_jobs }}"
            TOTAL_JOBS="${{ steps.scrape.outputs.jobs_found }}"
            echo "[SUCCESS] ${{ matrix.country.name }}: $TOTAL_JOBS new jobs (Software: $SOFTWARE_JOBS, HR: $HR_JOBS, Cybersecurity: $CYBER_JOBS, Sales: $SALES_JOBS, Finance: $FINANCE_JOBS)"
          else
            echo "[FAILED] ${{ matrix.country.name }}: Scraping failed"
          fi

  # Cleanup and Summary - runs after all countries finish OR if scraping was skipped
  cleanup:
    runs-on: ubuntu-latest
    needs: [check-scraping-needed, scrape-country]
    if: always()  # Run even if scraping was skipped or failed

    steps:
      - name: Check if scraping was skipped
        id: skip_check
        run: |
          if [ "${{ needs.check-scraping-needed.outputs.should_scrape }}" == "false" ]; then
            echo "Scraping was skipped due to no active users"
            echo "skipped=true" >> $GITHUB_OUTPUT
          else
            echo "skipped=false" >> $GITHUB_OUTPUT
          fi

      - name: Enforce country job limits
        if: steps.skip_check.outputs.skipped == 'false'
        id: cleanup
        run: |
          echo "Running cleanup..."
          RESPONSE=$(curl -s -X POST https://web-production-110bb.up.railway.app/api/jobs/enforce-country-limit \
            -H "Content-Type: application/json" \
            -d '{"max_jobs": 300}')
          echo "$RESPONSE" | python3 -m json.tool

          # Extract stats for notification
          JOBS_DELETED=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('jobs_deleted', 0))")
          TOTAL_JOBS=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('total_jobs_remaining', 0))")

          echo "jobs_deleted=$JOBS_DELETED" >> $GITHUB_OUTPUT
          echo "total_jobs=$TOTAL_JOBS" >> $GITHUB_OUTPUT

      - name: Get user activity
        id: stats
        run: |
          # Get user application activity (last 24h) - only active users
          USER_ACTIVITY=$(curl -s "https://web-production-110bb.up.railway.app/api/admin/user-activity")
          echo "$USER_ACTIVITY" | python3 -m json.tool

          # Parse and format user activity for Slack
          ACTIVITY_TEXT=$(echo "$USER_ACTIVITY" | python3 -c "
          import sys, json
          try:
              data = json.load(sys.stdin)
              users = data.get('users', [])
              if not users:
                  print('No user activity in last 24 hours')
              else:
                  for user in users:
                      applied = user.get('applied_24h', 0)
                      rejected = user.get('rejected_24h', 0)
                      if applied > 0 or rejected > 0:
                          print(f\"â€¢ *{user['username']}*: {applied} applied, {rejected} rejected\")
          except:
              print('Error fetching user activity')
          " || echo "â€¢ No recent activity")

          echo "user_activity<<EOF" >> $GITHUB_OUTPUT
          echo "$ACTIVITY_TEXT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Send consolidated Slack notification
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -z "$SLACK_WEBHOOK_URL" ]; then
            echo "SLACK_WEBHOOK_URL not configured, skipping notification"
            exit 0
          fi

          # Check if scraping was skipped
          if [ "${{ steps.skip_check.outputs.skipped }}" == "true" ]; then
            MESSAGE=$(cat <<'EOF'
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "â¸ï¸ Job Scraping Skipped"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Reason:* No active users found\n\nScraping was skipped to save resources. Activate users to resume automatic job scraping."
                  }
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "Manage Users"
                      },
                      "url": "https://web-production-110bb.up.railway.app/admin/users"
                    }
                  ]
                }
              ]
            }
          EOF
          )
          else
            # Normal success message
            MESSAGE=$(cat <<'EOF'
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "âœ… Job Scraping Complete"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Run Summary*\n:white_check_mark: Scraped for ${{ needs.check-scraping-needed.outputs.active_users }} active users\n:clock3: Duration: ~6 minutes"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {"type": "mrkdwn", "text": "*Total Jobs:*\n${{ steps.cleanup.outputs.total_jobs }}"},
                    {"type": "mrkdwn", "text": "*Cleaned:*\n${{ steps.cleanup.outputs.jobs_deleted }}"}
                  ]
                },
                {
                  "type": "divider"
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*User Activity (Last 24h)*\n${{ steps.stats.outputs.user_activity }}"
                  }
                },
                {
                  "type": "divider"
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Job Types Scraped:*\n${{ needs.check-scraping-needed.outputs.job_types }}\n\n*Countries Scraped:*\n${{ needs.check-scraping-needed.outputs.countries_json }}"
                  }
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "View Jobs"
                      },
                      "url": "https://web-production-110bb.up.railway.app"
                    }
                  ]
                }
              ]
            }
          EOF
          )
          fi

          curl -X POST "$SLACK_WEBHOOK_URL" \
            -H "Content-Type: application/json" \
            -d "$MESSAGE"

          echo "Notification sent to Slack"

      - name: Create summary
        run: |
          # Create GitHub Actions summary
          echo "### Job Scraping Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.skip_check.outputs.skipped }}" == "true" ]; then
            echo "#### â¸ï¸ Scraping Skipped" >> $GITHUB_STEP_SUMMARY
            echo "- **Reason:** No active users found" >> $GITHUB_STEP_SUMMARY
            echo "- **Active Users:** 0" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ’¡ **Tip:** Activate users in the admin panel to resume scraping" >> $GITHUB_STEP_SUMMARY
          else
            echo "#### âœ… Scraping Complete" >> $GITHUB_STEP_SUMMARY
            echo "- **Active Users:** ${{ needs.check-scraping-needed.outputs.active_users }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Jobs:** ${{ steps.cleanup.outputs.total_jobs }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Jobs Removed:** ${{ steps.cleanup.outputs.jobs_deleted }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Job Types:** ${{ needs.check-scraping-needed.outputs.job_types }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Countries:** ${{ needs.check-scraping-needed.outputs.countries_json }}" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Links" >> $GITHUB_STEP_SUMMARY
          echo "- [View Dashboard](https://web-production-110bb.up.railway.app)" >> $GITHUB_STEP_SUMMARY
          echo "- [Manage Users](https://web-production-110bb.up.railway.app/admin/users)" >> $GITHUB_STEP_SUMMARY

# Optimizations:
# - Checks active users BEFORE scraping (saves time if no active users)
# - Scraper internally filters by active user preferences
# - Skips entire workflow if no active users
# - Shows which job types/countries were actually scraped in Slack
